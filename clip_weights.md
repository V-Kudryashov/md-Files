Класс! Тогда --- конкретика по весам клипов и как ими «рулить», чтобы не
скатываться в один режим.

# Что можно «взвешивать»

1.  **Вероятности выборки клипа** при старте эпизода (sampling).
2.  **Вклад каждого клипа в r_t** (reward aggregation).
3.  **Вес сэмплов в лоссе** (loss reweighting / gradient balancing).
4.  **Архитектурные «веса»** (например, отдельные value-головы на клип).

Ниже --- работающие рецепты, их можно комбинировать.

------------------------------------------------------------------------

## 1) Статические веса выборки клипов

Простейшая мера, чтобы не «съехало» в Running: задать распределение
$p_i$ для клипа $i$ и **не** делать его равномерным.

-   Пример: $p_\text{Idle}=0.25$, $p_\text{Walk}=0.25$,
    $p_\text{Back}=0.2$, $p_\text{Run}=0.2$, $p_\text{Sprint}=0.1$.
-   Плюс: мгновенный эффект. Минус: не адаптируется под динамику
    обучения.

------------------------------------------------------------------------

## 2) Адаптивные веса выборки (контроль целевой «доли использования»)

Держим целевые доли $u_i^\*$ (например, равные или смещённые в сторону
сложных клипов) и онлайн-обновляем $p_i$, чтобы фактическая доля $u_i$ ≈
$u_i^\*$.

**Контроллер пропорционального типа:**

$$
p_i \leftarrow \text{softmax}\big(\log p_i + \eta \, (u_i^\* - u_i)\big)
$$

-   Обновляйте раз в N шагов (например, каждые 50k итераций);
    $\eta \in [0.5, 2.0]$.
-   Если видите, что Running доминирует (
    $u_\text{Run} > u_\text{Run}^\*$ ), формула «вытеснит» его
    вероятности в пользу других.

**Вариант с «трудностью»**: вместо $u_i$ используйте метрику
успеха/вознаграждения $s_i$ и двигайте $p_i \uparrow$ для тех, у кого
$s_i$ низкая (т.е. harder-first curriculum).

------------------------------------------------------------------------

## 3) Взвешивание reward по клипам (вместо max)

Если сейчас делаете $r_t=\max_i r_{i,t}$ --- это способствует mode
collapse. Поменяйте агрегацию:

**LogSumExp с температурой и весами:**

$$
r_t = \tau \,\log \sum_{i} w_i \exp\!\Big(\tfrac{r_{i,t}}{\tau}\Big)
$$

-   $\tau$ (temperature) сглаживает максимум: $\tau \in [0.1, 1.0]$.
-   $w_i$ --- ваши **клиповыe веса** (фиксированные или адаптивные).
-   Чем выше $w_i$, тем охотнее политика «смотрит» на соответствующий
    клип.

**Пороговый микс:**

$$
r_t = \sum_i w_i \,\mathbb{I}[r_{i,t} \ge r_{\text{top}} - \delta] \, r_{i,t}
$$

-   Берём все клипы, близкие к лучшему (within $\delta$), а не один
    лучший. Это резко снижает «залипание» на одном клипе.

**Нормализация по клипу**: обязательно держите для каждого клипа свой
running mean/std и используйте

$$
\tilde r_{i,t}=\frac{r_{i,t}-\mu_i}{\sigma_i+\epsilon}
$$

перед агрегацией (LogSumExp или пороговой). Иначе «более масштабный» по
значениям клип доминирует.

------------------------------------------------------------------------

## 4) Loss reweighting / баланс градиентов

Даже при честной выборке, один клип может «тянуть» обновления.

**GradNorm-подобный подход**: мониторьте норму градиента по мини-батчам
каждого клипа и динамически масштабируйте вклад в лосс, чтобы выровнять
нормы:

$$
\mathcal{L} = \sum_i \alpha_i \,\mathcal{L}_i,\quad
\alpha_i \propto \frac{\bar g}{g_i+\epsilon}
$$

где $g_i$ --- текущая норма градиента по клипу $i$, $\bar g$ ---
среднее. Это предотвращает «забивание» слабых клипов.

**Per-clip advantage norm**: нормируйте преимущества **по клипу**, а не
глобально. Это эквивалентно тому, чтобы каждый клип имел «свой масштаб
сигнала» в policy-градиенте.

------------------------------------------------------------------------

## 5) Архитектурные приёмы, дружелюбные к весам

-   **Shared actor + per-clip value heads**: один policy, но отдельные
    $V_i(s)$. Это сильно уменьшает взаимное «перетягивание» критика
    между навыками.
-   **Conditioning на skill-id/контекст скорости**: добавьте one-hot
    клипа или спид-таргет в вход актёру/критику. Весами выше управлять
    легче, потому что политика «знает», какой режим ожидается.
-   **Mixture-of-Experts**: эксперты на под-навыки, затвор (gating)
    получает мягкие приоритеты $w_i$. Дальше --- те же адаптивные
    обновления $w_i$.

------------------------------------------------------------------------

## 6) «Буст слабых клипов» (bandit-логика)

Трактуйте выбор клипа как **контекстно-связанную бандит-задачу**:

-   Наблюдаемый «возврат» для клипа $i$: $\hat J_i$ (скользящая средняя
    финального эпизодного reward или успеха).
-   Обновляйте $p_i$ с UCB/Thompson-подобной стратегией, **повышая**
    долю там, где недоучено.
-   Это надёжный способ «не дать» одному клипу съесть весь бюджет.

------------------------------------------------------------------------

## 7) KL-якорение к клиповым экспертам

Если у вас есть отдельно обученные политики $\pi_i$ (по одному на клип),
можно обучать общую $\pi$ с регуляризацией:

$$
\mathcal{L} \;+=\; \beta \,\mathbb{E}_{i\sim p_i,\,s\sim \pi}\Big[\,\mathrm{KL}\big(\pi_i(\cdot|s)\,\Vert\,\pi(\cdot|s, i)\big)\Big]
$$

-   $\beta$ небольшая (0.01--0.1). Это удерживает мультинейросеть от
    «забывания» менее выгодных режимов.

------------------------------------------------------------------------

## 8) Практические настройки (рабочий старт)

-   **Sampling**: целевые доли $u^\*$: Idle 0.2, Walk 0.25, Back 0.2,
    Run 0.2, Sprint 0.15. Контроллер $\eta=1.0$, апдейт каждые 50k
    шагов.
-   **Reward agg**: LogSumExp с $\tau=0.3$ и нач. $w_i = u_i^\*$.
    Обновляйте
    $w_i \leftarrow \text{softmax}(\log w_i + \lambda(u_i^\* - u_i))$,
    $\lambda=0.5$.
-   **Norm**: per-clip z-score для всех терминов imitation (pose,
    velocity, end-effector, root).
-   **Critic**: общий backbone + per-clip heads $V_i$.
-   **Entropy**: держите энтропию не ниже минимума (e.g.,
    $\mathcal{H}\ge h_\text{min}$), особенно в первые 3--5 млн шагов.
-   **Early curriculum**: на первых 1--2 млн шагов слегка завышайте
    $w_\text{Idle}, w_\text{Walk}, w_\text{Back}$, затем отпускайте к
    $u^\*$.

------------------------------------------------------------------------

## 9) Диагностика в TensorBoard

Добавьте графики:

-   Фактическая доля эпизодов по клипам $u_i$ и целевая $u_i^\*$.
-   Средний нормированный reward по клипам $\tilde r_i$.
-   Нормы градиентов по клипам (если используете GradNorm-подобное).
-   Успешность эпизодов и длина эпизодов по клипам.
-   KL к «экспертам» (если используете регуляризацию).

Если доминирование возобновляется --- уменьшайте $\tau$ (делайте мягкий
max «острее»), увеличивайте $\eta/\lambda$, поднимайте целевую долю
сложных клипов.

------------------------------------------------------------------------

## 10) Короткий псевдокод (идея)

``` python
# clip_probs: текущее p_i; target_usage: u_i*
for every K steps:
    usage = moving_average(episodes_per_clip / total_episodes)
    # контроллер по долям
    logits = np.log(clip_probs + 1e-8) + eta * (target_usage - usage)
    clip_probs = softmax(logits)

def aggregate_reward(r_per_clip, w, tau):
    # r_per_clip: dict {i: normalized_r_i_t}
    # LogSumExp с весами
    x = np.array([np.log(w[i]+1e-8) + r_per_clip[i]/tau for i in clips])
    return tau * logsumexp(x)

# В лоссе (PPO/A2C):
# - пер-клип нормировка преимуществ
# - GradNorm баланс для policy/critic по minibatch’ам каждого клипа
```

------------------------------------------------------------------------

### В двух словах

-   **Сгладьте max** (LogSumExp/пороговая смесь) и **нормируйте
    per-clip**.
-   **Управляйте вероятностями клипов** под целевые доли (контроллер).
-   **Балансируйте градиенты** и/или сделайте **per-clip value-головы**.
-   При необходимости **якорьте** к клиповым экспертам или используйте
    **bandit-логику**.

Если хотите, могу помочь адаптировать это под ваш стек (PPO/SAC,
PyTorch/TensorFlow) и предложить конкретные места патчей в коде
(инициализация буферов, где считать usage, как хранить $\mu_i,\sigma_i$
и т.д.).
